{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwv5LMsKhzTFQj66t8TeQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker0803r/deep-learning-ian-goodfellow/blob/master/reinforce/PPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "oLF8iYEC0Nss"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple,deque\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import gym\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "Memory = namedtuple('Memory', ['state', 'action', 'action_log_prob', 'reward', 'done', 'value'])\n",
        "AuxMemory = namedtuple('Memory', ['state', 'target_value', 'old_values'])\n",
        "memories = deque([])\n",
        "aux_memories = deque([])\n",
        "\n",
        "def init_(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        gain = torch.nn.init.calculate_gain('tanh')\n",
        "        torch.nn.init.orthogonal_(m.weight, gain)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "class ExperienceDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[0])\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        return tuple(map(lambda t: t[ind], self.data))\n",
        "\n",
        "def create_shuffled_dataloader(data, batch_size):\n",
        "    ds = ExperienceDataset(data)\n",
        "    return DataLoader(ds, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "def clipped_value_loss(values, rewards, old_values, clip):\n",
        "    value_clipped = old_values + (values - old_values).clamp(-clip, clip)\n",
        "    value_loss_1 = (value_clipped.flatten() - rewards) ** 2\n",
        "    value_loss_2 = (values.flatten() - rewards) ** 2\n",
        "    return torch.mean(torch.max(value_loss_1, value_loss_2))\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim, num_actions):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.action_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, num_actions),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.value_head = nn.Linear(hidden_dim, 1)\n",
        "        self.apply(init_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.net(x)\n",
        "        return self.action_head(hidden), self.value_head(hidden)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "        self.apply(init_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "  \n",
        "def normalize(t, eps = 1e-5):\n",
        "    return (t - t.mean()) / (t.std() + eps)\n",
        "\n",
        "def update_network_(loss, optimizer):\n",
        "    optimizer.zero_grad()\n",
        "    loss.mean().backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PPG(nn.Module):\n",
        "  def __init__(self,state_dim,num_actions,actor_hidden_dim,critic_hidden_dim,epochs,\n",
        "        epochs_aux,minibatch_size,lr,betas,lam,gamma,beta_s,eps_clip,value_clip,device):\n",
        "    super().__init__()\n",
        "    self.actor = Actor(state_dim, actor_hidden_dim, num_actions)\n",
        "    self.critic = Critic(state_dim, critic_hidden_dim)\n",
        "    self.device = device\n",
        "    self.gamma = gamma\n",
        "    self.lam = lam\n",
        "    self.minibatch_size = minibatch_size\n",
        "    self.eps_clip = eps_clip\n",
        "    self.beta_s = beta_s\n",
        "    self.opt_actor = torch.optim.Adam(self.actor.parameters(),lr=lr)\n",
        "    self.opt_critic = torch.optim.Adam(self.critic.parameters(),lr=lr)\n",
        "    self.value_clip = value_clip\n",
        "    self.epochs = epochs\n",
        "    self.epochs_aux = epochs_aux\n",
        "\n",
        "  \n",
        "  def load(self):\n",
        "    self.load_state_dict(torch.load('model.pt'))\n",
        "  \n",
        "  def save(self):\n",
        "    torch.save(self.state_dict(),'model.pt')\n",
        "  \n",
        "  def learn(self, memories, aux_memories, next_state):\n",
        "        states = []\n",
        "        actions = []\n",
        "        old_log_probs = []\n",
        "        rewards = []\n",
        "        masks = []\n",
        "        values = []\n",
        "\n",
        "        for mem in memories:\n",
        "            states.append(mem.state)\n",
        "            actions.append(torch.tensor(mem.action))\n",
        "            old_log_probs.append(mem.action_log_prob)\n",
        "            rewards.append(mem.reward)\n",
        "            # invert done for GAE calculations\n",
        "            masks.append(1 - float(mem.done))\n",
        "            values.append(mem.value)\n",
        "\n",
        "        # calculate generalized advantage estimate\n",
        "        next_state = torch.from_numpy(next_state).to(self.device)\n",
        "        next_value = self.critic(next_state).detach()\n",
        "        values = values + [next_value]\n",
        "\n",
        "        returns = []\n",
        "        gae = 0\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            delta = rewards[i] + self.gamma * values[i + 1] * masks[i] - values[i]\n",
        "            gae = delta + self.gamma * self.lam * masks[i] * gae\n",
        "            returns.insert(0, gae + values[i])\n",
        "\n",
        "        # convert values to torch tensors\n",
        "        to_torch_tensor = lambda t: torch.stack(t).to(self.device).detach()\n",
        "\n",
        "        states = to_torch_tensor(states)\n",
        "        actions = to_torch_tensor(actions)\n",
        "        old_values = to_torch_tensor(values[:-1])\n",
        "        old_log_probs = to_torch_tensor(old_log_probs)\n",
        "\n",
        "        rewards = torch.tensor(returns).float().to(self.device)\n",
        "\n",
        "        # store state and target values to auxiliary memory buffer for later training\n",
        "        aux_memory = AuxMemory(states, rewards, old_values)\n",
        "        aux_memories.append(aux_memory)\n",
        "\n",
        "        # prepare dataloader for policy phase training\n",
        "        dl = create_shuffled_dataloader([states, actions, old_log_probs, rewards, old_values], self.minibatch_size)\n",
        "\n",
        "        # policy phase training, similar to original PPO\n",
        "        for _ in range(self.epochs):\n",
        "            for states, actions, old_log_probs, rewards, old_values in dl:\n",
        "                action_probs, _ = self.actor(states)\n",
        "                values = self.critic(states)\n",
        "                dist = Categorical(action_probs)\n",
        "                action_log_probs = dist.log_prob(actions)\n",
        "                entropy = dist.entropy()\n",
        "\n",
        "                # calculate clipped surrogate objective, classic PPO loss\n",
        "                ratios = (action_log_probs - old_log_probs).exp()\n",
        "                advantages = normalize(rewards - old_values.detach())\n",
        "                surr1 = ratios * advantages\n",
        "                surr2 = ratios.clamp(1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "                policy_loss = - torch.min(surr1, surr2) - self.beta_s * entropy\n",
        "\n",
        "                update_network_(policy_loss, self.opt_actor)\n",
        "\n",
        "                # calculate value loss and update value network separate from policy network\n",
        "                value_loss = clipped_value_loss(values, rewards, old_values, self.value_clip)\n",
        "\n",
        "                update_network_(value_loss, self.opt_critic)\n",
        "  def learn_aux(self, aux_memories):\n",
        "        states = []\n",
        "        rewards = []\n",
        "        old_values = []\n",
        "        for state, reward, old_value in aux_memories:\n",
        "            states.append(state)\n",
        "            rewards.append(reward)\n",
        "            old_values.append(old_value)\n",
        "\n",
        "        states = torch.cat(states)\n",
        "        rewards = torch.cat(rewards)\n",
        "        old_values = torch.cat(old_values)\n",
        "\n",
        "        # get old action predictions for minimizing kl divergence and clipping respectively\n",
        "        old_action_probs, _ = self.actor(states)\n",
        "        old_action_probs.detach_()\n",
        "\n",
        "        # prepared dataloader for auxiliary phase training\n",
        "        dl = create_shuffled_dataloader([states, old_action_probs, rewards, old_values], self.minibatch_size)\n",
        "\n",
        "        # the proposed auxiliary phase training\n",
        "        # where the value is distilled into the policy network, while making sure the policy network does not change the action predictions (kl div loss)\n",
        "        for epoch in range(self.epochs_aux):\n",
        "            for states, old_action_probs, rewards, old_values in dl:\n",
        "                action_probs, policy_values = self.actor(states)\n",
        "                action_logprobs = action_probs.log()\n",
        "\n",
        "                # policy network loss copmoses of both the kl div loss as well as the auxiliary loss\n",
        "                aux_loss = clipped_value_loss(policy_values, rewards, old_values, self.value_clip)\n",
        "                loss_kl = F.kl_div(action_logprobs, old_action_probs, reduction='batchmean')\n",
        "                policy_loss = aux_loss + loss_kl\n",
        "\n",
        "                update_network_(policy_loss, self.opt_actor)\n",
        "\n",
        "                # paper says it is important to train the value network extra during the auxiliary phase\n",
        "                values = self.critic(states)\n",
        "                value_loss = clipped_value_loss(values, rewards, old_values, self.value_clip)\n",
        "\n",
        "                update_network_(value_loss, self.opt_critic)        "
      ],
      "metadata": {
        "id": "GLPBlkvEwkq5"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(\n",
        "    env_name = 'LunarLander-v2',\n",
        "    num_episodes = 5000,\n",
        "    max_timesteps = 500,\n",
        "    actor_hidden_dim = 32,\n",
        "    critic_hidden_dim = 256,\n",
        "    minibatch_size = 16,\n",
        "    lr = 0.0005,\n",
        "    betas = (0.9, 0.999),\n",
        "    lam = 0.95,\n",
        "    gamma = 0.99,\n",
        "    eps_clip = 0.2,\n",
        "    value_clip = 0.4,\n",
        "    beta_s = .01,\n",
        "    update_timesteps = 64,\n",
        "    num_policy_updates_per_aux = 32,\n",
        "    epochs = 1,\n",
        "    epochs_aux = 6,\n",
        "    render = False,\n",
        "    render_every_eps = 250,\n",
        "    save_every = 1000,\n",
        "    load = True,\n",
        "    print_reward_interval = 10,\n",
        "):\n",
        "    \"\"\"\n",
        "    :param env_name: OpenAI gym environment name\n",
        "    :param num_episodes: number of episodes to train\n",
        "    :param max_timesteps: max timesteps per episode\n",
        "    :param actor_hidden_dim: actor network hidden layer size\n",
        "    :param critic_hidden_dim: critic network hidden layer size\n",
        "    :param minibatch_size: minibatch size for training\n",
        "    :param lr: learning rate for optimizers\n",
        "    :param betas: betas for Adam Optimizer\n",
        "    :param lam: GAE lambda (exponential discount)\n",
        "    :param gamma: GAE gamma (future discount)\n",
        "    :param eps_clip: PPO policy loss clip coefficient\n",
        "    :param value clip: value loss clip coefficient\n",
        "    :param beta_s: entropy loss coefficient\n",
        "    :param update_timesteps: number of timesteps to run before training\n",
        "    :param epochs: policy phase epochs\n",
        "    :param epochs_aux: auxiliary phase epochs\n",
        "    :param render: toggle render environment\n",
        "    :param render_every_eps: if render, how often to render\n",
        "    :param save_every: how often to save networks\n",
        "    :load: toggle load a previously trained network \n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    memories = deque([])\n",
        "    aux_memories = deque([])\n",
        "\n",
        "    agent = PPG(\n",
        "        state_dim,\n",
        "        num_actions,\n",
        "        actor_hidden_dim,\n",
        "        critic_hidden_dim,\n",
        "        epochs,\n",
        "        epochs_aux,\n",
        "        minibatch_size,\n",
        "        lr,\n",
        "        betas,\n",
        "        lam,\n",
        "        gamma,\n",
        "        beta_s,\n",
        "        eps_clip,\n",
        "        value_clip,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    if load:\n",
        "      pass\n",
        "        #agent.load()\n",
        "\n",
        "    time = 0\n",
        "    updated = False\n",
        "    num_policy_updates = 0\n",
        "\n",
        "    for eps in range(num_episodes):\n",
        "        render_eps = render and eps % render_every_eps == 0\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        for timestep in range(max_timesteps):\n",
        "            time += 1\n",
        "\n",
        "            if updated and render_eps:\n",
        "                env.render()\n",
        "\n",
        "            state = torch.from_numpy(state).to(device)\n",
        "            action_probs, _ = agent.actor(state)\n",
        "            value = agent.critic(state)\n",
        "\n",
        "            dist = Categorical(action_probs)\n",
        "            action = dist.sample()\n",
        "            action_log_prob = dist.log_prob(action)\n",
        "            action = action.item()\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            memory = Memory(state, action, action_log_prob, reward, done, value)\n",
        "            memories.append(memory)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if time % update_timesteps == 0:\n",
        "                agent.learn(memories, aux_memories, next_state)\n",
        "                num_policy_updates += 1\n",
        "                memories.clear()\n",
        "\n",
        "                if num_policy_updates % num_policy_updates_per_aux == 0:\n",
        "                    agent.learn_aux(aux_memories)\n",
        "                    aux_memories.clear()\n",
        "\n",
        "                updated = True\n",
        "\n",
        "            if done:\n",
        "              if eps % print_reward_interval == 0:\n",
        "                print(f'eps:{eps},total_reward:{total_reward}')\n",
        "                if render_eps:\n",
        "                    updated = False\n",
        "                break\n",
        "\n",
        "        if render_eps:\n",
        "            env.close()\n",
        "\n",
        "        if eps % save_every == 0:\n",
        "            pass\n",
        "            #agent.save()"
      ],
      "metadata": {
        "id": "TmjwKthkuzMs"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tCwIQvHEvg8N",
        "outputId": "12f03c2a-23c0-4abd-ab24-cbe53c427ffe"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eps:0,total_reward:-457.2370217378628\n",
            "eps:10,total_reward:-265.042451966741\n",
            "eps:20,total_reward:-192.21099659424584\n",
            "eps:30,total_reward:-321.47092543806605\n",
            "eps:40,total_reward:-548.6910798843332\n",
            "eps:50,total_reward:-314.5578492951288\n",
            "eps:60,total_reward:-561.3409298841557\n",
            "eps:70,total_reward:-96.72455662811723\n",
            "eps:80,total_reward:-120.39051650666418\n",
            "eps:90,total_reward:-62.14781111608708\n",
            "eps:100,total_reward:-435.59147498118887\n",
            "eps:110,total_reward:-252.59948826203248\n",
            "eps:120,total_reward:-87.01209656226551\n",
            "eps:130,total_reward:-176.4662142309512\n",
            "eps:140,total_reward:-342.8118258754097\n",
            "eps:150,total_reward:-126.2154492097739\n",
            "eps:160,total_reward:-336.39268060387974\n",
            "eps:170,total_reward:-68.57622235573072\n",
            "eps:180,total_reward:10.616886590529504\n",
            "eps:190,total_reward:-335.5122101800707\n",
            "eps:200,total_reward:-452.6678194020224\n",
            "eps:210,total_reward:-404.0946033801291\n",
            "eps:220,total_reward:-103.24625469870553\n",
            "eps:230,total_reward:-89.47769911975224\n",
            "eps:240,total_reward:-155.5899793045744\n",
            "eps:250,total_reward:-126.49137726991786\n",
            "eps:260,total_reward:-153.93630197005456\n",
            "eps:270,total_reward:-236.91476015279298\n",
            "eps:280,total_reward:-308.86029782558705\n",
            "eps:290,total_reward:-227.8290476614162\n",
            "eps:300,total_reward:-367.52158696977494\n",
            "eps:310,total_reward:-33.100125747595996\n",
            "eps:320,total_reward:-220.72475714887537\n",
            "eps:330,total_reward:-103.20012736138735\n",
            "eps:340,total_reward:-163.27157568147865\n",
            "eps:350,total_reward:-318.9219676361978\n",
            "eps:360,total_reward:-131.6771380956337\n",
            "eps:370,total_reward:-212.77993692168843\n",
            "eps:380,total_reward:-286.8208152711212\n",
            "eps:390,total_reward:-244.6893392122821\n",
            "eps:400,total_reward:-180.0096998006671\n",
            "eps:410,total_reward:-92.71498272495845\n",
            "eps:420,total_reward:-234.92823001412484\n",
            "eps:430,total_reward:-111.4473811896837\n",
            "eps:440,total_reward:-242.77339192139564\n",
            "eps:450,total_reward:-111.25903795936154\n",
            "eps:460,total_reward:-99.05955991821476\n",
            "eps:470,total_reward:-105.2139904690497\n",
            "eps:480,total_reward:-154.92264028966687\n",
            "eps:490,total_reward:-316.05578454091255\n",
            "eps:500,total_reward:-240.54639194615015\n",
            "eps:510,total_reward:-37.74935936700102\n",
            "eps:520,total_reward:-236.59913209372408\n",
            "eps:530,total_reward:-4.231440495634857\n",
            "eps:540,total_reward:-267.44769178087836\n",
            "eps:550,total_reward:-224.97151098125318\n",
            "eps:560,total_reward:-78.67554735955233\n",
            "eps:570,total_reward:-35.76927897420788\n",
            "eps:580,total_reward:-157.29965556696382\n",
            "eps:590,total_reward:-79.03603239900082\n",
            "eps:600,total_reward:-102.31193835279132\n",
            "eps:610,total_reward:-127.14357387648958\n",
            "eps:620,total_reward:-46.158989495838824\n",
            "eps:630,total_reward:-326.2985677911019\n",
            "eps:640,total_reward:-106.31639261171709\n",
            "eps:650,total_reward:-65.3771710922885\n",
            "eps:660,total_reward:-273.31738540854263\n",
            "eps:670,total_reward:-188.94221213085063\n",
            "eps:680,total_reward:-197.50392506233277\n",
            "eps:690,total_reward:-98.48672854648322\n",
            "eps:700,total_reward:-60.770223985402474\n",
            "eps:710,total_reward:7.862405677789596\n",
            "eps:720,total_reward:-64.6849827640414\n",
            "eps:730,total_reward:-246.1300225438101\n",
            "eps:740,total_reward:-155.83949177860646\n",
            "eps:750,total_reward:-77.41766044543122\n",
            "eps:760,total_reward:-95.81922280592468\n",
            "eps:770,total_reward:-65.7049430727974\n",
            "eps:780,total_reward:-396.4790019175038\n",
            "eps:790,total_reward:-179.5322558965454\n",
            "eps:800,total_reward:-174.65711211537206\n",
            "eps:810,total_reward:-287.9562769435963\n",
            "eps:820,total_reward:-186.87971695327298\n",
            "eps:830,total_reward:-60.56334087719663\n",
            "eps:840,total_reward:-266.2943849180606\n",
            "eps:850,total_reward:-77.61307167531075\n",
            "eps:860,total_reward:-247.20362421292168\n",
            "eps:870,total_reward:-99.78246947075371\n",
            "eps:880,total_reward:-126.73927394985557\n",
            "eps:890,total_reward:-219.41502310798865\n",
            "eps:900,total_reward:-39.16953142798073\n",
            "eps:910,total_reward:-130.16259896151928\n",
            "eps:920,total_reward:-315.5669294657217\n",
            "eps:930,total_reward:-193.24668594908732\n",
            "eps:940,total_reward:-247.3653739154166\n",
            "eps:950,total_reward:-157.83348795920847\n",
            "eps:960,total_reward:-365.0698499791931\n",
            "eps:970,total_reward:-196.8298799469361\n",
            "eps:980,total_reward:-417.56034220157255\n",
            "eps:990,total_reward:-242.257069531407\n",
            "eps:1000,total_reward:-53.38709296077119\n",
            "eps:1010,total_reward:-338.5385106787677\n",
            "eps:1020,total_reward:-310.10028474532453\n",
            "eps:1030,total_reward:-291.2775407725816\n",
            "eps:1040,total_reward:-142.1860553916946\n",
            "eps:1050,total_reward:-141.07009260960427\n",
            "eps:1060,total_reward:-72.30056001086733\n",
            "eps:1070,total_reward:-299.6405345930947\n",
            "eps:1080,total_reward:-130.955605518197\n",
            "eps:1090,total_reward:-127.68149054736996\n",
            "eps:1100,total_reward:-110.42563604915617\n",
            "eps:1110,total_reward:-178.80887300962442\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-137-f25227048910>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(env_name, num_episodes, max_timesteps, actor_hidden_dim, critic_hidden_dim, minibatch_size, lr, betas, lam, gamma, eps_clip, value_clip, beta_s, update_timesteps, num_policy_updates_per_aux, epochs, epochs_aux, render, render_every_eps, save_every, load, print_reward_interval)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_policy_updates\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_policy_updates_per_aux\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_aux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux_memories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                     \u001b[0maux_memories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-136-45d387392bdd>\u001b[0m in \u001b[0;36mlearn_aux\u001b[0;34m(self, aux_memories)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclipped_value_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mupdate_network_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-135-7a644b15bdc8>\u001b[0m in \u001b[0;36mupdate_network_\u001b[0;34m(loss, optimizer)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asrH9ijRvouV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}